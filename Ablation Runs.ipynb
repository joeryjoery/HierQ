{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload dependencies upon changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-century",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import product, count\n",
    "from copy import deepcopy\n",
    "import typing\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mazelab_experimenter import EnvRegister\n",
    "from mazelab_experimenter import AgentShelve\n",
    "from mazelab_experimenter import benchmark, GenericOuterHook, PredictionErrorHook, apply_trace\n",
    "from mazelab_experimenter.utils import compute_optimal_value\n",
    "\n",
    "\n",
    "# Result output format.\n",
    "FILE_FORMAT = 'pdf'\n",
    "OUTPUT_FORMAT = './figures/ablation_{}.' + FILE_FORMAT\n",
    "\n",
    "print(\"Output figures will be formatted as:\", OUTPUT_FORMAT.format(\"FILENAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-robinson",
   "metadata": {},
   "source": [
    "## Environment Definition\n",
    "\n",
    "We explore simple gridworld environments presenting tabular features to the agents. We test our agents on an empty world and on a world with rooms, we keep the dimensionality small due to memory requirements incurred by either the hindsight tables or eligibility trace models.\n",
    "\n",
    "The agents/ environments are initialized with a binary reward function, i.e., a function that always returns $0$ until a desired state is reached upon which it returns $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 100_000  # Number of steps before terminating an episode for any agent on all environments.\n",
    "\n",
    "name_grid10 = EnvRegister.register(\n",
    "    maze_type='gridworld', \n",
    "    name=f'gridworld-v0', \n",
    "    env_args=dict(\n",
    "        binary_rewards=True\n",
    "    ),\n",
    "    generator_args=dict(height=12, width=12),\n",
    "    initialization_args=dict(\n",
    "        start_pos=[[1, 1]], goal_pos=[[10, 10]]\n",
    "    ),\n",
    "    time_limit=t,\n",
    "    override=True\n",
    ")\n",
    "\n",
    "name_grid20 = EnvRegister.register(\n",
    "    maze_type='gridworld', \n",
    "    name=f'gridworld-v1', \n",
    "    env_args=dict(\n",
    "        binary_rewards=True\n",
    "    ),\n",
    "    generator_args=dict(height=22, width=22),\n",
    "    initialization_args=dict(\n",
    "        start_pos=[[1, 1]], goal_pos=[[20, 20]]\n",
    "    ),\n",
    "    time_limit=t,\n",
    "    override=True\n",
    ")\n",
    "\n",
    "name_rooms4 = EnvRegister.register(\n",
    "    maze_type='n_rooms_square', \n",
    "    name=f'n_rooms-v0', \n",
    "    env_args=dict(\n",
    "        binary_rewards=True\n",
    "    ),\n",
    "    generator_args=dict(num_rooms=4, room_size=5, gap_size=1),\n",
    "    initialization_args=dict(\n",
    "        start_pos=[[1, 1]], goal_pos=[[7, 7]]\n",
    "    ),\n",
    "    time_limit=t,\n",
    "    override=True\n",
    ")\n",
    "\n",
    "name_rooms9 = EnvRegister.register(\n",
    "    maze_type='n_rooms_square', \n",
    "    name=f'n_rooms-v1', \n",
    "    env_args=dict(\n",
    "        binary_rewards=True\n",
    "    ),\n",
    "    generator_args=dict(num_rooms=9, room_size=5, gap_size=1),\n",
    "    initialization_args=dict(\n",
    "        start_pos=[[1, 1]], goal_pos=[[13, 13]]\n",
    "    ),\n",
    "    time_limit=t,\n",
    "    override=True\n",
    ")\n",
    "\n",
    "name_mazeS = EnvRegister.register(\n",
    "    maze_type='fixed_maze', \n",
    "    name=f'maze-v0', \n",
    "    env_args=dict(\n",
    "        binary_rewards=True\n",
    "    ),\n",
    "    generator_args=dict(large=False),\n",
    "    initialization_args=dict(\n",
    "        start_pos=[[1, 1]], goal_pos=[[11, 11]]\n",
    "    ),\n",
    "    time_limit=t,\n",
    "    override=True\n",
    ")\n",
    "\n",
    "name_mazeL = EnvRegister.register(\n",
    "    maze_type='fixed_maze', \n",
    "    name=f'maze-v1', \n",
    "    env_args=dict(\n",
    "        binary_rewards=True\n",
    "    ),\n",
    "    generator_args=dict(large=True),\n",
    "    initialization_args=dict(\n",
    "        start_pos=[[1, 1]], goal_pos=[[19, 19]]\n",
    "    ),\n",
    "    time_limit=t,\n",
    "    override=True\n",
    ")\n",
    "\n",
    "# Collect each individual environment.\n",
    "environment_names = [name_grid10, name_grid20, name_rooms4, name_rooms9, name_mazeS, name_mazeL]\n",
    "environment_labels = ['10x10 Gridworld', '20x20 Gridworld', '4-rooms 5-to-1', '9-rooms 5-to-1', '10x10 Maze', '20x20 Maze']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and show an example visualization of the environment\n",
    "dummy_images = list()\n",
    "for name in environment_names:\n",
    "    dummy = gym.make(name)\n",
    "    dummy.reset()\n",
    "    \n",
    "    dummy_images.append(dummy.unwrapped.get_image())\n",
    "    dummy.close()\n",
    "    \n",
    "fig, ax = plt.subplots(1, len(environment_names), figsize=(5 * len(environment_names), 5))\n",
    "for i, img in enumerate(dummy_images):\n",
    "    # Plot image with border padding\n",
    "    ax[i].imshow(img)\n",
    "    \n",
    "    # Grid lines\n",
    "    major, minor = np.arange(0, len(img), 1), np.arange(-.5, len(img), 1)\n",
    "    ax[i].set_xticks(major); ax[i].set_yticks(major)            # Major plot ticks\n",
    "    ax[i].set_xticklabels(major); ax[i].set_yticklabels(major)  # Major plot tick-labels\n",
    "    ax[i].set_xticks(minor, minor=True); ax[i].set_yticks(minor, minor=True)  # Minor ticks for Grid-lines.\n",
    "    \n",
    "    ax[i].grid(which='minor', color='black', linestyle='-', linewidth=.5)\n",
    "    \n",
    "    # Annotations\n",
    "    ax[i].set_title(environment_labels[i])\n",
    "\n",
    "plt.suptitle(\"Benchmark environments.\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(OUTPUT_FORMAT.format('BenchmarkEnvironment', FILE_FORMAT), format=FILE_FORMAT, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-edgar",
   "metadata": {},
   "source": [
    "## Benchmark Functionality\n",
    "\n",
    "For running the experiments, here we define helper functions that run various agent ablations over a variety of experimental parameters.\n",
    "\n",
    "This includes functionality for logging the approximation error to the optimal Q-table. Every experiment makes use of the outlined interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prediction_hook(_env: gym.Env, agent, solution: np.ndarray = None) -> typing.List[PredictionErrorHook]:\n",
    "    \"\"\" Helper function to construct a Q-max prediction error logger for a HierQV2 based agent. \"\"\"\n",
    "    if solution is None:\n",
    "        solution = compute_optimal_value(_env)\n",
    "        \n",
    "    aggr = lambda arr, i: [i, np.mean(np.square(arr)), np.mean(np.abs(arr))]\n",
    "    \n",
    "    references = list()\n",
    "    for i in range(agent.n_levels):    \n",
    "        h_a = agent.atomic_horizons[i]\n",
    "        \n",
    "        solution_i = solution // h_a\n",
    "        reference_i = agent.discount ** solution_i \n",
    "        \n",
    "        reference_i[solution == 0] = 0\n",
    "    \n",
    "        references.append(reference_i)\n",
    "    \n",
    "    hooks = list()\n",
    "    for i in range(agent.n_levels):\n",
    "        getter = lambda agent, i: agent.critics[i].table[:, (agent._get_index(_env.unwrapped.maze.get_end_pos()[0]) \n",
    "                                                             if agent.critics[i].goal_conditioned else 0), :].max(axis=-1)\n",
    "        \n",
    "        hooks.append(PredictionErrorHook(references[i], partial(getter, i=i), partial(aggr, i=i)))\n",
    "        \n",
    "    return hooks\n",
    "\n",
    "\n",
    "def run_benchmark(r: int, it: int, eps: int, trials: int, benchmark_environments: typing.List[str], \n",
    "                  ablation_generator: typing.Callable, verbose: bool = True, \n",
    "                  **kwargs) -> typing.List[typing.Dict]:\n",
    "    \"\"\" Helper function for running train-test experiments for numerous agents over a variety of experimental settings \"\"\"\n",
    "    benchmark_data = list()\n",
    "    for env_name in benchmark_environments:\n",
    "        print(\"=\" * 50 + \"\\nExperiment environment:\", env_name, end='\\n\\n')\n",
    "        meta, agent_gens, separators = ablation_generator(env_name)\n",
    "\n",
    "        env_data = list()\n",
    "        for i, (m, gen_a) in enumerate(zip(meta, agent_gens)):\n",
    "            print(f\"Benchmarking agent: {i+1}/{len(meta)} - ablation:\", m)\n",
    "\n",
    "            # Run the benchmark\n",
    "            env_data.append(\n",
    "                benchmark(\n",
    "                    env_id=env_name, \n",
    "                    _agent_gen=gen_a, \n",
    "                    agent_test_kwargs=dict(behaviour_policy=False),\n",
    "                    agent_train_kwargs=dict(),\n",
    "                    skip_random_evaluation=True,\n",
    "                    num_repetitions=r,\n",
    "                    num_iterations=it,\n",
    "                    num_episodes=eps,\n",
    "                    num_trials=trials,\n",
    "                    evaluation_hooks=[GenericOuterHook()],\n",
    "                    verbose=verbose,\n",
    "                    asynchronous=False,\n",
    "                    **kwargs\n",
    "                )\n",
    "            )\n",
    "\n",
    "        benchmark_data.append({'name': env_name, 'meta': meta, 'data': env_data, 'separators': separators})\n",
    "        \n",
    "    return benchmark_data\n",
    "\n",
    "\n",
    "\n",
    "def run_episode(r: int, benchmark_environments: typing.List[str], ablation_generator: typing.Callable, \n",
    "                verbose: bool = True, **kwargs) -> typing.List[typing.Dict]:\n",
    "    \"\"\" Helper function for training numerous agents once over a variety of experimental settings \"\"\"\n",
    "    benchmark_data = list()\n",
    "    for env_name in benchmark_environments:\n",
    "        print(\"=\" * 50 + \"\\nExperiment environment:\", env_name, end='\\n\\n')\n",
    "        meta, agent_gens, separators = ablation_generator(env_name)\n",
    "\n",
    "        env_data = list()\n",
    "        for i, (m, gen_a) in enumerate(zip(meta, agent_gens)):\n",
    "            print(f\"Train agent: {i+1}/{len(meta)} - ablation:\", m)\n",
    "            \n",
    "            rep_data = list()\n",
    "            for _ in tqdm.trange(r):\n",
    "                env = gym.make(env_name)\n",
    "                agent = gen_a()\n",
    "\n",
    "                agent.reset()\n",
    "                agent.train(env, 1, False)\n",
    "                rep_data.append(env.steps)\n",
    "\n",
    "            env_data.append(rep_data)\n",
    "                \n",
    "        benchmark_data.append({'name': env_name, 'meta': meta, 'data': env_data, 'separators': separators})\n",
    "        \n",
    "    return benchmark_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-paragraph",
   "metadata": {},
   "source": [
    "## Experiment Performance Multi-Step Returns: Agent Definition \n",
    "\n",
    "For benchmarking the agents we utilize two types of implemented classes for either the flat or the hierarchical agents. These are the atomic and compound update types, note that an atomic 1-step update $\\equiv$ to a $\\lambda=0$ compound update which is just 1-step $TD$. For $\\lambda=0$ we can fall back to the multi-step implementation with $n=1$ for simplicity, we test out both methods to assert empirical equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared Keyword arguments\n",
    "shared_kwargs = {\n",
    "    'epsilon': 0.25,\n",
    "    'lr': 1.0\n",
    "}\n",
    "\n",
    "# Defaults for Hierarchical agents.\n",
    "hierarchical_defaults = {\n",
    "    'horizons': 3\n",
    "}\n",
    "\n",
    "\n",
    "def create_agent_generator(_env: gym.Env, atomic: bool, **kwargs) -> typing.Callable:\n",
    "    \"\"\" Helper function for instantiating agents with a fixed goal given some environment. \n",
    "    \n",
    "    Atomic implies n-step returns, otherwise implies compound/ lambda returns.\n",
    "    Hierarchical denotes whether to use a hierarchical Q-learner or a flat agent.\n",
    "    \n",
    "    Actual agent hyperparameters 'v' should be specified outside this function.\n",
    "    \"\"\"\n",
    "    agent = AgentShelve.retrieve(\n",
    "        agent=('HierQN' if atomic else 'HierQLambda'),  \n",
    "        keyword_arguments=dict(\n",
    "            observation_shape=_env.observation_space.shape,\n",
    "            n_actions=_env.action_space.n,\n",
    "            **kwargs, **hierarchical_defaults, **shared_kwargs\n",
    "        ))\n",
    "    agent.set_goal(agent._get_index(_env.unwrapped.maze.get_end_pos()[0]))   \n",
    "    return agent\n",
    "\n",
    "\n",
    "def get_environment_agent_ablations(_env_name: str, ablation_grid: typing.Dict) -> typing.List[typing.Generator]:\n",
    "    \"\"\" Helper function for creating agent generators with an environment dependency over a grid of parameters. \"\"\"\n",
    "    dummy = gym.make(_env_name)\n",
    "    \n",
    "    # Unpack all ablation-runs over their parameter grid into flat lists.\n",
    "    meta, agent_generators, separators = list(), list(), list()\n",
    "    for name, ablations in ablations_grid.items():\n",
    "        # Take cartesian product of all specified parameter options.\n",
    "        keys, values = zip(*ablations.items())\n",
    "        grid = [dict(zip(keys, v)) for v in product(*values)]\n",
    "        \n",
    "        # Initialize agents.\n",
    "        for p in grid:\n",
    "            gen = partial(create_agent_generator, _env=dummy, **p)\n",
    "       \n",
    "            # Add agent generator and ablation metadata to global execution list.        \n",
    "            agent_generators.append(gen)\n",
    "            meta.append(p)\n",
    "        \n",
    "        # Append grid length to separate list for slicing convenience.\n",
    "        separators.append(len(grid))\n",
    "        \n",
    "    return meta, agent_generators, separators\n",
    "        \n",
    "\n",
    "# Define (ad-hoc) Parameter Ablation Grid and wrap into generator function.\n",
    "ablations_grid = {\n",
    "    'lambda': {\n",
    "        'atomic': [False], 'discount': [0.95],'decay': [0.0, 0.5, 0.8, 1.0], \n",
    "        'n_levels': [4, 3, 2, 1], 'greedy_options': [True], 'flat_training': [True, False]},\n",
    "    'multistep_hierarchical': {\n",
    "        'atomic': [True], 'discount': [0.95], \n",
    "        'n_steps': [1, 3, 5, 8], 'n_levels': [4, 3, 2, 1], 'greedy_options': [True], 'flat_training': [True, False]}\n",
    "}\n",
    "# Use this function with an environment arguments to get all agents specified by the parameter grid as generators.\n",
    "# >> meta, agents, seps = get_environment_ablations(env_name)\n",
    "get_environment_ablations = partial(get_environment_agent_ablations, ablation_grid=ablations_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-yugoslavia",
   "metadata": {},
   "source": [
    "### Run Benchmark & IO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd5a207",
   "metadata": {},
   "source": [
    "Collect number of steps needed in first training episode to observe first environment reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7cd69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = 200\n",
    "\n",
    "for e in environment_names:  # The small environments\n",
    "    \n",
    "    results = run_episode(\n",
    "        r=r,\n",
    "        benchmark_environments=[e],  \n",
    "        ablation_generator=get_environment_ablations,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    with open(f't100k_first_reward_ablation_{e}_{int(time.time())}.out', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-bridal",
   "metadata": {},
   "source": [
    "**Run Small Environments**\n",
    "\n",
    "Collect performance over multiple training episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-tourism",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = 200\n",
    "\n",
    "for e in environment_names[::2]:  # The small environments\n",
    "    # Benchmark configuration and execution.\n",
    "    small_benchmark_data = run_benchmark(\n",
    "        r=r,\n",
    "        it=50,\n",
    "        eps=1,\n",
    "        trials=1,\n",
    "        benchmark_environments=[e],  \n",
    "        ablation_generator=get_environment_ablations,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    with open(f't100kperformance_ablation_{e}_{int(time.time())}.out', 'wb') as f:\n",
    "        pickle.dump(small_benchmark_data, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-gender",
   "metadata": {},
   "source": [
    "**Run Large Environments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-danger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = 200\n",
    "\n",
    "for e in environment_names[1::2]:  # The Large environments\n",
    "    # Benchmark configuration and execution.\n",
    "    large_benchmark_data = run_benchmark(\n",
    "        r=r,\n",
    "        it=50,\n",
    "        eps=1,\n",
    "        trials=1,\n",
    "        benchmark_environments=[e], \n",
    "        ablation_generator=get_environment_ablations,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    with open(f'performance_ablation_{e}_{int(time.time())}.out', 'wb') as f:\n",
    "        pickle.dump(large_benchmark_data, f)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf9175",
   "metadata": {},
   "source": [
    "## Discount Balancing HierQ Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration reference: 2-level hierarchy with H=3 and discount=0.95\n",
    "base_discount = 0.95\n",
    "base_horizon = 3.0\n",
    "base_nstep = 3\n",
    "base_level = 2.0\n",
    "\n",
    "# Grid for balancing effective credit assignment depth for different hierarchy levels:\n",
    "levels = np.asarray([1, 2, 3])\n",
    "\n",
    "gammas = base_discount ** (base_horizon ** (levels - base_level))\n",
    "nsteps = base_nstep // (base_horizon ** (levels - base_level))\n",
    "print(\"Adjusted discounts to balance reward propagation depth:\\n\", *zip(levels, gammas.round(4)))\n",
    "print(\"Adjusted multistep to balance reward propagation depth:\\n\", *zip(levels, nsteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared Keyword arguments\n",
    "shared_kwargs = {\n",
    "    'epsilon': 0.25,\n",
    "    'lr': 1.0\n",
    "}\n",
    "\n",
    "# Defaults for Hierarchical agents.\n",
    "hierarchical_defaults = {\n",
    "    'horizons': 3\n",
    "}\n",
    "\n",
    "# Generate Parameter Ablation Grid based on adjusted discount-parameters per hierarchy level, and wrap into generator function.\n",
    "ablations_grid = dict()\n",
    "\n",
    "base_lambda = {'flat_training': [True, False], 'atomic': [False], 'decay': [1.0], 'greedy_options': [True]}\n",
    "base_nstep = {'flat_training': [True, False], 'atomic': [True], 'greedy_options': [True]}\n",
    "for i, discount, n_adjusted in zip(levels, gammas, nsteps):\n",
    "    p = {'discount': [discount], 'n_levels': [i]}\n",
    "    \n",
    "    l, n = base_lambda.copy(), base_nstep.copy()\n",
    "    \n",
    "    l.update(p); n.update(p)\n",
    "    n['n_steps'] = [int(n_adjusted)]\n",
    "    \n",
    "    if i == 1:\n",
    "        l['flat_training'] = [True]\n",
    "        n['flat_training'] = [True]\n",
    "    \n",
    "    ablations_grid[f'multistep_i={i}'] = n\n",
    "    ablations_grid[f'lambda_i={i}'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703b197",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for correctness\n",
    "ablations_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e621d15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this function with an environment arguments to get all agents specified by the parameter grid as generators.\n",
    "# >> meta, agents, seps = get_environment_ablations(env_name)\n",
    "get_environment_ablations = partial(get_environment_agent_ablations, ablation_grid=ablations_grid)\n",
    "\n",
    "r = 200\n",
    "\n",
    "for e in environment_names:  # The Large environments\n",
    "    # Benchmark configuration and execution.\n",
    "    large_benchmark_data = run_benchmark(\n",
    "        r=r,\n",
    "        it=50,\n",
    "        eps=1,\n",
    "        trials=1,\n",
    "        benchmark_environments=[e], \n",
    "        ablation_generator=get_environment_ablations,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    with open(f't100k-full-discount-performance_ablation_{e}_{int(time.time())}.out', 'wb') as f:\n",
    "        pickle.dump(large_benchmark_data, f)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
